{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "coco2017_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l9IQxr2jWaui",
        "Lp4s5Q5fdG_x",
        "AJ-lfNPQlcuO",
        "Uara-01_5NIB",
        "zPcMtX-sxDW7",
        "-PhLjHCmTpDS"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joangog/object-detection/blob/main/coco2017_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIjcsMslUx9m"
      },
      "source": [
        "## Model evaluation (inference) on COCO 2017 dataset\n",
        "\n",
        "The following models will be evaluated:\n",
        "\n",
        "| Model | Backbone | Image Size | Parameters | GFLOPS \n",
        "| --- | --- | --- | --- | --- |\n",
        "| SSD300 | VGG16 | 300x300 | 35.6M | 69.8 |\n",
        "| SSDlite320 | MobileNetV3-Large | 320x320 | 3.4M | 1.2 |\n",
        "| Faster R-CNN |  ResNet-50 FPN || 41.8M |  |\n",
        "| Faster R-CNN |  MobileNetV3-Large FPN || 19.4M |  |\n",
        "| Mask R-CNN |  ResNet-50 FPN || 44.4M | |\n",
        "| YOLOv5s |  Custom | 640x640 | 7.3M | 17 |\n",
        "| YOLOv5m |  Custom | 640x640 | 21.4M | 51.3 |\n",
        "| YOLOv5l |  Custom |640x640 | 47M | 115.5 |\n",
        "| YOLOv3 |  Darknet53 | 640x640 | 70M | 156.3 |\n",
        "| YOLOv3-tiny |  Darknet53 | 640x640 | 8.8M | 13.3 |\n",
        "| YOLOv3-spp |  Darknet53 | 640x640 | 63M | 157.1 |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note: GPU Runtime needed**\n",
        "\n",
        "*Example experiment: Tesla K80, 460.32.03, 11441 MiB, batch_size=8, workers=2*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X0kW-fHdG9R",
        "outputId": "8046e404-d4ce-48ff-8b66-fd1a04370e7a"
      },
      "source": [
        "# Show system specs\n",
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name, driver_version, memory.total [MiB]\n",
            "Tesla K80, 460.32.03, 11441 MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9IQxr2jWaui"
      },
      "source": [
        "### Get requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-XSXNc61vV3"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install Yolov5\n",
        "cd /content\n",
        "git clone https://github.com/ultralytics/yolov5\n",
        "cd yolov5\n",
        "pip install --quiet -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYO5_FOUR1D_",
        "outputId": "b63f0ae7-f37b-4fb9-dc30-c7d4e887f768"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install Yolov3\n",
        "cd /content\n",
        "git clone https://github.com/ultralytics/yolov3\n",
        "cd yolov3\n",
        "pip install --quiet -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'yolov3' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVa0AxzKlSyv"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install flops-counter\n",
        "pip install ptflops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gWOqjlQQdaP"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Clone asset files\n",
        "cd /content\n",
        "git clone https://github.com/joangog/object-detection-assets\n",
        "cd object-detection-assets\n",
        "mv scripts ../\n",
        "rm -rf /content/object-detection-assets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp4s5Q5fdG_x"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZg3dOcWfsV"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import gdown\n",
        "\n",
        "import os, sys\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import PIL\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models.detection as M\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.utils as U\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "import scripts.utils as SU\n",
        "import scripts.transforms as ST\n",
        "import scripts.coco_utils as SCU\n",
        "from scripts.coco_eval import CocoEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ-lfNPQlcuO"
      },
      "source": [
        "### Define auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FW_LJclb-P"
      },
      "source": [
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, M.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, M.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = SU.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "    coco = SCU.get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    # Get label names\n",
        "    label_ids = data_loader.dataset.coco.getCatIds()\n",
        "    label_info = data_loader.dataset.coco.loadCats(label_ids)\n",
        "    label_names = [label['name'] for label in label_info]\n",
        "    labels = dict(zip(label_ids, label_names))  # Label dictionary with id-name as key-value\n",
        "    labels_inv = dict(zip(label_names, label_ids))  # Inverse label dictionary with name-id as key-value\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "\n",
        "        images = list(img.to(device) for img in images)\n",
        "        if model.__class__.__name__ == 'AutoShape':  # If model is from YOLOv5 package\n",
        "            images = [F.to_pil_image(image) for image in images]  # Convert images from tensor to PIL\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)  # Get model predictions\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        if model.__class__.__name__ == 'AutoShape':  # If model is from YOLO package\n",
        "            # Format outputs to COCO format\n",
        "            outputs_formatted = []\n",
        "            for img_outputs in outputs.xyxy:\n",
        "                output_bboxes = img_outputs[:, :4]\n",
        "                output_scores = img_outputs[:, 4]\n",
        "                output_labels = img_outputs[:, 5].to(cpu_device).apply_(\n",
        "                    lambda x: labels_inv[label_names[int(x)]])  # Convert YOLO label ids to COCO label ids\n",
        "                outputs_formatted.append({\n",
        "                    'boxes': output_bboxes,\n",
        "                    'scores': output_scores,\n",
        "                    'labels': output_labels\n",
        "                })\n",
        "            outputs = outputs_formatted\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "\n",
        "        # test lines #######################\n",
        "\n",
        "        # label_ids = dataset.coco.getCatIds()\n",
        "        # label_info = dataset.coco.loadCats(label_ids)\n",
        "        # label_names = [label['name'] for label in label_info]\n",
        "        # labels = dict(zip(label_ids,label_names))\n",
        "\n",
        "        # img = F.convert_image_dtype(images[1],torch.uint8).cpu()\n",
        "\n",
        "        # true_bboxes = F.Tensor([obj['bbox'] for obj in targets[1]]).cpu()\n",
        "        # true_labels = [labels[obj['category_id']] for obj in targets[1]]\n",
        "        # true_img = U.draw_bounding_boxes(img, true_bboxes, true_labels)\n",
        "        # plt.figure(figsize = (25,7))\n",
        "        # plt.imshow(F.to_pil_image(true_img))\n",
        "\n",
        "        # output = outputs[1]\n",
        "        # pred_bboxes = torch.stack([output['boxes'][i] for i in range(0,len(output['boxes'])) if output['scores'][i] > th])\n",
        "        # pred_labels_ids = output['labels'].tolist()\n",
        "        # pred_label_ids = [pred_labels_ids[i] for i in range(0,len(pred_labels_ids)) if output['scores'][i] > th]\n",
        "        # pred_labels = [labels[label_id] for label_id in pred_label_ids]\n",
        "        # pred_img = U.draw_bounding_boxes(img, pred_bboxes, pred_labels)\n",
        "        # plt.figure(figsize = (25,7))\n",
        "        # plt.imshow(F.to_pil_image(pred_img))\n",
        "\n",
        "        # fig()\n",
        "\n",
        "        ###############\n",
        "\n",
        "        res = {target[0][\"image_id\"]: output for target, output in zip(targets, outputs) if len(target) != 0}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "\n",
        "    # Model FPS\n",
        "    batch_size = data_loader.batch_size\n",
        "    fps = batch_size / metric_logger.meters['model_time'].global_avg\n",
        "\n",
        "    return coco_evaluator, fps, outputs\n",
        "\n",
        "def convert_to_xyxy(bboxes):  # formats bboxes from (x,y,w,h) to (x,y,x,y)\n",
        "  for bbox in bboxes:\n",
        "    bbox[2] = bbox[0] + bbox[2]\n",
        "    bbox[3] = bbox[1] + bbox[3]\n",
        "  return bboxes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFpoV3E0-smW"
      },
      "source": [
        "### Download COCO 2017 validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGZH-ZSzu6nK"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6rHTrcZ1Wle"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/dataset\n",
        "\n",
        "# Download images\n",
        "wget 'http://images.cocodataset.org/zips/val2017.zip'\n",
        "unzip -q 'val2017.zip'\n",
        "rm 'val2017.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIvrZuC_1YK0"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/dataset\n",
        "\n",
        "# Download annotations\n",
        "wget 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "unzip -q 'annotations_trainval2017.zip'\n",
        "rm 'annotations_trainval2017.zip'\n",
        "cp '/content/dataset/annotations/instances_val2017.json' '/content/dataset/val2017'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uara-01_5NIB"
      },
      "source": [
        "### Load COCO 2017 validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC4qQcYh5R88"
      },
      "source": [
        "img_dir = '/content/dataset/val2017'\n",
        "ann_file = 'instances_val2017.json'  # annotations\n",
        "ann_path = os.path.join(img_dir,ann_file)\n",
        "\n",
        "# Define data transforms\n",
        "transforms = ST.Compose([ST.ToTensor()])\n",
        "\n",
        "# Create dataset\n",
        "dataset = CocoDetection(img_dir, ann_path, transforms = transforms)\n",
        "\n",
        "# Create data loader\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=SU.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwGEoyOZfKq"
      },
      "source": [
        "### Load pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVJqPlW8dTvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e4fc02d2-2ecb-468c-e522-c2e977c5fb72"
      },
      "source": [
        "%cd /content\n",
        "\n",
        "# Delete utils package to reload it (if loaded), because YOLOv3 and YOLOv5 have\n",
        "# the same name for it and it causes error\n",
        "try:\n",
        "  sys.modules.pop('utils')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# @markdown Model Selection { display-mode: 'form', run: 'auto' }\n",
        "model_name = 'YOLOv3-spp' # @param ['SSD300 VGG16', 'SSDlite320 MobileNetV3-Large', 'Faster R-CNN ResNet-50 FPN', 'Faster R-CNN MobileNetV3-Large FPN', 'Mask R-CNN ResNet-50 FPN', 'YOLOv5s', 'YOLOv5m', 'YOLOv5l', 'YOLOv3', 'YOLOv3-tiny', 'YOLOv3-spp']\n",
        "\n",
        "# @markdown *Note: If you get the error \"Cache may be out of date, try 'force_reload=True'\" then restart runtime.*\n",
        "\n",
        "if model_name == 'SSD300 VGG16':\n",
        "  model_id = 'ssd300_vgg16'\n",
        "  model = M.ssd300_vgg16(pretrained=True, progress=True)\n",
        "  model_img_size = (3,300,300)\n",
        "elif model_name == 'SSDlite320 MobileNetV3-Large':\n",
        "  model_id = 'ssdlite320_mobilenet_v3_large'\n",
        "  model = M.ssdlite320_mobilenet_v3_large(pretrained=True, progress=True)\n",
        "  model_img_size = (3,320,320)\n",
        "elif model_name == 'Faster R-CNN ResNet-50 FPN':\n",
        "  model_id = 'fasterrcnn_resnet50_fpn'\n",
        "  model = M.fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,1333,1333) \n",
        "elif model_name == 'Faster R-CNN MobileNetV3-Large FPN':\n",
        "  model_id = 'fasterrcnn_mobilenet_v3_large_fpn'\n",
        "  model = M.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,224,224) \n",
        "elif model_name == 'Mask R-CNN ResNet-50 FPN':\n",
        "  model_id = 'maskrcnn_resnet50_fpn'\n",
        "  model = M.maskrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,224,224)\n",
        "elif model_name == 'YOLOv5s':\n",
        "  model_id = 'yolov5s'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "  model_img_size = (3,640,640)\n",
        "elif model_name == 'YOLOv5m':\n",
        "  model_id = 'yolov5m'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5m')\n",
        "  model_img_size = (3,640,640)\n",
        "elif model_name == 'YOLOv5l':\n",
        "  model_id = 'yolov5l'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5l')\n",
        "  model_img_size = (3,640,640)\n",
        "elif model_name == 'YOLOv3':\n",
        "  model_id = 'yolov3'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3', force_reload=True)\n",
        "  model_img_size = (3,640,640)\n",
        "elif model_name == 'YOLOv3-tiny':\n",
        "  model_id = 'yolov3_tiny'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3_tiny')\n",
        "  model_img_size = (3,640,640)\n",
        "elif model_name == 'YOLOv3-spp':\n",
        "  model_id = 'yolov3_spp'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3_spp')\n",
        "  model_img_size = (3,640,640)\n",
        "\n",
        "print('-------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "print(f'Loaded model: {model_name}')\n",
        "model_params = sum([param.numel() for param in model.parameters()])\n",
        "print(f'\\t- Parameters: {round(model_params / 1000000, 1)}M')\n",
        "model_macs, _ = get_model_complexity_info(model, model_img_size, as_strings=False, \n",
        "                                          print_per_layer_stat=False, verbose=False)\n",
        "model_flops = 2 * int(model_macs)\n",
        "print(f'\\t- GFLOPS: {round(model_flops / 1000000000, 1)}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov3_master\n",
            "Fusing layers... \n",
            "Model Summary: 269 layers, 62971933 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "YOLOv3 🚀 2021-8-27 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loaded model: YOLOv3-spp\n",
            "\t- Parameters: 63.0M\n",
            "\t- GFLOPS: 157.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPcMtX-sxDW7"
      },
      "source": [
        "### (Optional) Test model with image sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRnDnuP-xC99"
      },
      "source": [
        "# Parameters\n",
        "img_id = 139\n",
        "th = 0.5  # threshold for confidence score of predicted bboxes to show\n",
        "\n",
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Get image sample\n",
        "img = PIL.Image.open(os.path.join(img_dir,dataset.coco.loadImgs([img_id])[0]['file_name']))\n",
        "img_tensor = F.convert_image_dtype(F.to_tensor(img),torch.uint8)\n",
        "img_torchvision = torch.div(img_tensor,255).float().to(device)  # Format image for torchvision models\n",
        "img_anns = dataset.coco.loadAnns(dataset.coco.getAnnIds([img_id]))\n",
        "\n",
        "# Generate model predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  if 'YOLO' in model_name:\n",
        "    pred = model([img])\n",
        "  else:    \n",
        "    pred = model([img_torchvision])\n",
        "\n",
        "# Get label names\n",
        "label_ids = dataset.coco.getCatIds()\n",
        "label_info = dataset.coco.loadCats(label_ids)\n",
        "label_names = [label['name'] for label in label_info]\n",
        "labels = dict(zip(label_ids,label_names))  # Label dictionary with id-name as key-value\n",
        "labels_inv = dict(zip(label_names,label_ids))  # Inverse label dictionary with name-id as key-value\n",
        "\n",
        "# Get ground truth bboxes\n",
        "true_bboxes = convert_to_xyxy(copy.deepcopy(F.Tensor([obj['bbox'] for obj in img_anns]).to(device)))  # Create deep copy to avoid updating original dataset\n",
        "true_labels = [labels[obj['category_id']] for obj in img_anns]\n",
        "true_img = U.draw_bounding_boxes(img_tensor, true_bboxes, true_labels)\n",
        "plt.figure(figsize = (25,7))\n",
        "plt.title('Ground Truth Detection')\n",
        "plot = plt.imshow(F.to_pil_image(true_img))\n",
        "\n",
        "# Get ground truth segmentation masks\n",
        "# print(img_anns)\n",
        "# true_masks = F.Tensor([obj['segmentation'][0] for obj in img_anns])\n",
        "# true_labels = [labels[obj['category_id']] for obj in img_anns]\n",
        "# true_img = U.draw_segmentation_masks(img, true_masks, true_labels)\n",
        "# plt.figure(figsize = (25,7))\n",
        "# plt.imshow(F.to_pil_image(true_img))\n",
        "\n",
        "# Get predicted bboxes\n",
        "# For YOLO models\n",
        "if 'YOLO' in model_name:  \n",
        "  pred_bboxes = []\n",
        "  pred_labels = []\n",
        "  for img in pred.xyxy:\n",
        "    for bbox in img:\n",
        "      if bbox[4] > th:  # Show only bboxes with high confidence score\n",
        "        pred_bboxes.append(bbox[:4])\n",
        "        pred_labels.append(labels[labels_inv[label_names[int(bbox[5])]]])  # Convert YOLO label to COCO label\n",
        "  pred_bboxes = torch.stack(pred_bboxes)\n",
        "# For torchvision models\n",
        "else:\n",
        "  pred_bboxes = torch.stack([pred[0]['boxes'][i] for i in range(0,len(pred[0]['boxes'])) if pred[0]['scores'][i] > th])  # Show only bboxes with high confidence score\n",
        "  pred_labels_ids = torch.stack([pred[0]['labels'][i] for i in range(0,len(pred[0]['labels'])) if pred[0]['scores'][i] > th]).tolist()\n",
        "  pred_labels = [labels[label_id] for label_id in pred_labels_ids]\n",
        "pred_img = U.draw_bounding_boxes(img_tensor, pred_bboxes, pred_labels)\n",
        "plt.figure(figsize = (25,7))\n",
        "plt.title(f'Predicted Detection (thresh={th})')\n",
        "plot = plt.imshow(F.to_pil_image(pred_img))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4E0DKJ4bOg"
      },
      "source": [
        "### Evaluate model on dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BViZf4Ud4Vrp",
        "outputId": "0ccfc1f8-19e3-456c-cd26-5cc640259bb3"
      },
      "source": [
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "print(f'Model: {model_name}')\n",
        "\n",
        "# Evaluate model\n",
        "evaluator, fps, outputs = evaluate(model, data_loader, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: YOLOv3-spp\n",
            "Test:  [  0/625]  eta: 0:14:10  model_time: 0.9027 (0.9027)  evaluator_time: 0.0279 (0.0279)  time: 1.3607  data: 0.3321  max mem: 775\n",
            "Test:  [100/625]  eta: 0:07:49  model_time: 0.7590 (0.7548)  evaluator_time: 0.0313 (0.0324)  time: 0.8980  data: 0.0170  max mem: 775\n",
            "Test:  [200/625]  eta: 0:06:20  model_time: 0.7600 (0.7541)  evaluator_time: 0.0350 (0.0353)  time: 0.9046  data: 0.0183  max mem: 775\n",
            "Test:  [300/625]  eta: 0:04:50  model_time: 0.7637 (0.7547)  evaluator_time: 0.0277 (0.0340)  time: 0.8882  data: 0.0167  max mem: 775\n",
            "Test:  [400/625]  eta: 0:03:21  model_time: 0.7671 (0.7554)  evaluator_time: 0.0263 (0.0333)  time: 0.9042  data: 0.0161  max mem: 775\n",
            "Test:  [500/625]  eta: 0:01:51  model_time: 0.7656 (0.7556)  evaluator_time: 0.0248 (0.0342)  time: 0.8806  data: 0.0156  max mem: 775\n",
            "Test:  [600/625]  eta: 0:00:22  model_time: 0.7657 (0.7557)  evaluator_time: 0.0313 (0.0337)  time: 0.8801  data: 0.0156  max mem: 775\n",
            "Test:  [624/625]  eta: 0:00:00  model_time: 0.7633 (0.7560)  evaluator_time: 0.0275 (0.0336)  time: 0.8911  data: 0.0147  max mem: 775\n",
            "Test: Total time: 0:09:19 (0.8945 s / it)\n",
            "Averaged stats: model_time: 0.7633 (0.7560)  evaluator_time: 0.0275 (0.0336)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=3.13s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.381\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.540\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.416\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.433\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.305\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.235\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PhLjHCmTpDS"
      },
      "source": [
        "### Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8L2RtxdkDVz"
      },
      "source": [
        "cols = ['model', 'model_params', 'fps', 'iou_type', 'metric', 'iou', 'area', 'max_dets', 'score']\n",
        "model = []\n",
        "model_params_arr = []\n",
        "fps_arr = []\n",
        "iou_type = []\n",
        "metric = []\n",
        "iou = []\n",
        "area = []\n",
        "max_dets = []\n",
        "score = []\n",
        "\n",
        "# Set column values\n",
        "for curr_iou_type, coco_eval in evaluator.coco_eval.items():\n",
        "  model += [model_name for i in range(0,12)]\n",
        "  model_params_arr += [model_params for i in range(0,12)]\n",
        "  fps_arr += [fps for i in range(0,12)]\n",
        "  iou_type += [curr_iou_type for i in range(0,12)]\n",
        "  metric += ['avg_precision' for i in range(0,6)] + ['avg_recall' for i in range(0,6)]\n",
        "  iou += ['0.50:0.95', '0.50 ', '0.75'] + ['0.50:0.95' for i in range(0,9)]\n",
        "  area += ['all' for i in range(0,3)] + ['small', 'medium', 'large'] + ['all' for i in range(0,3)] + ['small', 'medium', 'large'] \n",
        "  max_dets += [100 for i in range(0,6)] + [1, 10] + [100 for i in range(0,4)]\n",
        "  score += list(coco_eval.stats)\n",
        "\n",
        "results = pd.DataFrame(np.column_stack([model, model_params_arr, fps_arr, iou_type, metric, iou, area, max_dets, score]))\n",
        "results.columns = cols\n",
        "\n",
        "# Safe file\n",
        "results_file = f'coco17_{model_id}_results.csv'\n",
        "results_dir = '/content'\n",
        "results_path = os.path.join(results_dir, results_file)\n",
        "if os.path.exists(results_path):\n",
        "      os.remove(results_path)\n",
        "with open(results_path, 'w') as outfile: \n",
        "    results.to_csv(outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eEGk6qj2D4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348e5819-e663-425f-9bc8-456d398ef8ee"
      },
      "source": [
        "# Save file to GDrive\n",
        "drive.mount('/content/drive')\n",
        "gdrive_results_dir = '/content/drive/MyDrive/object-detection-outputs/coco2017'\n",
        "gdrive_results_path = os.path.join(gdrive_results_dir, results_file)\n",
        "if os.path.exists(gdrive_results_path):\n",
        "      os.remove(gdrive_results_path)\n",
        "with open(gdrive_results_path, 'w') as outfile: \n",
        "    results.to_csv(outfile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY3apojFTt2f"
      },
      "source": [
        "# results = []\n",
        "# for i, img_id in enumerate(img_ids):  # For every image\n",
        "#   img_predictions = predictions[i]\n",
        "#   pred_boxes = img_predictions['boxes'].tolist()\n",
        "#   pred_labels = img_predictions['labels'].tolist()\n",
        "#   pred_scores = img_predictions['scores'].tolist()\n",
        "#   for j in range(0,len(pred_box)):  # For every predicted object\n",
        "#     pred_box = pred_boxes[j]\n",
        "#     results.append({\n",
        "#         'image_id': img_id,\n",
        "#         'category_id': pred_labels[j],\n",
        "#         'bbox': [round(val,1) for val in pred_box],  # Round for lower file size\n",
        "#         'score': pred_scores[j]\n",
        "#         })\n",
        "\n",
        "# results_file = os.path.join(img_dir,f'{model_id}_coco17_results.json')\n",
        "# with open(results_file, 'w') as outfile: \n",
        "#     json.dump(results, outfile,indent = 2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
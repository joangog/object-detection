{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l9IQxr2jWaui",
        "Lp4s5Q5fdG_x",
        "a7yurBDzbVwE",
        "6hBNZ1E5oHv2",
        "kFpoV3E0-smW",
        "Uara-01_5NIB"
      ],
      "authorship_tag": "ABX9TyN1RFUbJjARxXh/79pef2SN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joangog/object-detection/blob/main/mask_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOnnttZXkP41"
      },
      "source": [
        "## Model evaluation (inference) on COCO 2017 dataset\n",
        "\n",
        "The following models will be evaluated:\n",
        "\n",
        "| Model | Backbone | Image Size | Parameters | GFLOPs\n",
        "| --- | --- | --- | --- | --- |\n",
        "| YOLOv5s |  Custom | 640x640 | 7.3M | 17 |\n",
        "| YOLOv5m |  Custom | 640x640 | 21.4M | 51.3 |\n",
        "| YOLOv5l |  Custom |640x640 | 47M | 115.5 |\n",
        "| YOLOv3-tiny |  Darknet53 | 640x640 | 8.8M | 13.3 |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note: GPU Runtime needed (hosted or local)**\n",
        "\n",
        "*Example experiment: Tesla K80, 460.32.03, 11441 MiB, batch_size=8, workers=2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X0kW-fHdG9R"
      },
      "source": [
        "# Show system specs\n",
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVojvH9i8ua0"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZHe4nxx8raL"
      },
      "source": [
        "# Parameters\n",
        "\n",
        "dataset_name = 'PWMFD'  # Dataset to evaluate: 'MASKD' or 'PWMFD'\n",
        "\n",
        "num_workers = 2  #  Data loader workers\n",
        "batch_size = 1  # Data loader batch size\n",
        "\n",
        "th = 0.5  # Threshold for confidence score of predicted bboxes to show\n",
        "\n",
        "# Directories\n",
        "\n",
        "load_ckpt_path = f'/home/ioanna/object-detection-checkpoints/pretrained/PWMFD_yolov5s_sgd_ep50_lr01_img320_run/weights/last.pt'  # Model checkpoint to load\n",
        "\n",
        "import os\n",
        "root_dir = os.getcwd()  # Root dir of project\n",
        "dataset_dir = os.path.join(root_dir,f'dataset_{dataset_name}')\n",
        "\n",
        "img_dir = os.path.join(dataset_dir,'images')\n",
        "val_img_dir = os.path.join(img_dir,'val_images')\n",
        "\n",
        "label_dir = os.path.join(dataset_dir,'labels')\n",
        "val_label_dir = os.path.join(label_dir,'val_images')\n",
        "\n",
        "ann_dir = os.path.join(dataset_dir,'annotations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9IQxr2jWaui"
      },
      "source": [
        "### Get requirements\n",
        "*Note: Restart runtime after installation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-XSXNc61vV3"
      },
      "source": [
        "# Install Yolov5\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!pip install -r {os.path.join(root_dir,'yolov5','requirements.txt')}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYO5_FOUR1D_"
      },
      "source": [
        "# Install Yolov3\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/ultralytics/yolov3\n",
        "!pip install -r {os.path.join(root_dir,'yolov3','requirements.txt')}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khDSIlMPgGAz"
      },
      "source": [
        "# Install colabtools\n",
        "!git clone https://github.com/googlecolab/colabtools.git\n",
        "!python {root_dir}/colabtools/setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ6TTWucuWo8"
      },
      "source": [
        "# Install unrar command\n",
        "if os.geteuid() != 0:  # If not root, ask for sudo priviledges\n",
        "  from getpass import getpass\n",
        "  password = getpass('Insert sudo password:')\n",
        "  !echo {password} | sudo -S -k apt-get install unrar\n",
        "else:\n",
        "  !apt-get install unrar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gWOqjlQQdaP"
      },
      "source": [
        "# Clone asset files\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/joangog/object-detection-assets\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','scripts')} ./\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','config')} ./\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','requirements.txt')} ./\n",
        "!rm -rf {os.path.join(root_dir,'object-detection-assets')}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVa0AxzKlSyv"
      },
      "source": [
        "# Install packages\n",
        "!cd {root_dir}\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp4s5Q5fdG_x"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZg3dOcWfsV"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "import os, sys\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import re\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import html\n",
        "\n",
        "import PIL\n",
        "import cv2\n",
        "import IPython\n",
        "from IPython.display import display, Javascript, Image\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "import torchvision.models.detection as M\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.utils as U\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "from pycocotools import coco\n",
        "from pycocotools import mask as cocomask\n",
        "\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "import scripts.utils as SU\n",
        "import scripts.transforms as ST\n",
        "import scripts.engine as SE\n",
        "import scripts.coco_utils as SCU\n",
        "from scripts.coco_eval import CocoEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7yurBDzbVwE"
      },
      "source": [
        "### Define auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r91k652pbYE7"
      },
      "source": [
        "def add_motion_blur(img, kernel_size, kernel_angle):\n",
        "  kernel = np.zeros((kernel_size, kernel_size))\n",
        "  if kernel_angle == 'v':  # Vertical Motion Blur\n",
        "    kernel[:, int((kernel_size - 1)/2)] = np.ones(kernel_size)\n",
        "  elif kernel_angle == 'h':  # Horizontal Motion Blur\n",
        "    kernel[int((kernel_size - 1)/2), :] = np.ones(kernel_size)\n",
        "  kernel /= kernel_size\n",
        "  return cv2.filter2D(img, -1, kernel)\n",
        "\n",
        "# Converts base64 image from JS reply to PIL image\n",
        "def js_to_image(js_reply):  \n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  img = PIL.Image.open(io.BytesIO(image_bytes))\n",
        "  return img\n",
        "\n",
        "# Convert OpenCV Rectangle bbox overlay image into base64 byte string\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  # Convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # Format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes\n",
        "\n",
        "# JavaScript functions to print live video stream using webcam\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>FPS: </span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBNZ1E5oHv2"
      },
      "source": [
        "### (Optional) Connect to GDrive for storage access\n",
        "*Note: Not possible with local runtime*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUb3wQZX9R8"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFpoV3E0-smW"
      },
      "source": [
        "### Download Mask dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZmlJ1DpK3ZG"
      },
      "source": [
        "!cd {root_dir}\n",
        "!mkdir -p dataset_{dataset_name}\n",
        "!cd {dataset_dir}\n",
        "!mkdir -p {img_dir} {ann_dir} {label_dir}\n",
        "!cd {img_dir}\n",
        "!mkdir {val_img_dir}\n",
        "!cd {label_dir}\n",
        "!mkdir {val_label_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLsRgTRRuu3J"
      },
      "source": [
        "if dataset_name == 'MASKD':\n",
        "\n",
        "  !cd {root_dir}\n",
        "\n",
        "  # Download validation images\n",
        "  if not os.path.exists('val_images.zip'):\n",
        "    !gdown --id '101F2k6PJ-tD_uwlsCG7zzGF9ILJW01M1'\n",
        "  !unzip -q -n 'val_images.zip' -d {img_dir}\n",
        "\n",
        "  # Download validation annotations\n",
        "  if not os.path.exists('val.json'):\n",
        "    !gdown -O {os.path.join(ann_dir,'val.json')} --id '1YLV7-7vmiNdFI8Xpdx_jbhnxfgQRWrgF'\n",
        "\n",
        "elif dataset_name == 'PWMFD':\n",
        "\n",
        "  # Download validation images\n",
        "  if not os.path.exists('val_images.rar'):\n",
        "    !gdown -O 'val_images.rar' --id  1ZXuSwoRvTnnca81RUj3kMoLFZJ6auAwT\n",
        "  !unrar e -idq -o- 'val_images.rar' -d {val_img_dir}\n",
        "\n",
        "  # Convert annotation files from PASCAL VOC .xml to COCO .json (only for PWMFD dataset)\n",
        "  \n",
        "  label_ids = {'with_mask': 1, 'without_mask': 2, 'incorrect_mask': 3}  # BG class is 0\n",
        "\n",
        "  ann_count = 0  # Annotation counter\n",
        "\n",
        "  images = []\n",
        "  annotations = []\n",
        "\n",
        "  xml_files = os.listdir(val_img_dir)\n",
        "  xml_files = [file for file in xml_files if '.xml' in file]\n",
        "\n",
        "  for xml_file in xml_files:\n",
        "\n",
        "    tree = ET.parse(os.path.join(val_img_dir,xml_file))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Image\n",
        "    file_name = root[0].text\n",
        "    height = int(root[1][1].text)\n",
        "    width = int(root[1][0].text)\n",
        "    id = int(re.sub(r'^\\D*0*', '', file_name).replace('.jpg',''))\n",
        "    images.append(\n",
        "        {\n",
        "          'file_name': file_name,\n",
        "          'height': height,\n",
        "          'width': width,\n",
        "          'id': id\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Annotations\n",
        "    if len(root) > 2:  # If annotations (object attribute) exist, they will be after the 1-index attribute in the XML\n",
        "      for i in range(2,len(root)):\n",
        "        category_id = label_ids[root[i][0].text]\n",
        "        xmin = int(root[i][1][0].text)\n",
        "        ymin = int(root[i][1][1].text)\n",
        "        xmax = int(root[i][1][2].text)\n",
        "        ymax = int(root[i][1][3].text)\n",
        "        annotations.append(\n",
        "            {\n",
        "              'iscrowd': 0,\n",
        "              'image_id': id,\n",
        "              'bbox': [xmin, ymin, xmax-xmin, ymax-ymin],\n",
        "              'area': (xmax-xmin) * (ymax-ymin),\n",
        "              'category_id': category_id,\n",
        "              'ignore': 0,\n",
        "              'id': ann_count,\n",
        "              'segmentation': []\n",
        "            }\n",
        "        )\n",
        "        ann_count += 1\n",
        "\n",
        "  coco_dict = {\n",
        "  'info': {},\n",
        "  'images': images,\n",
        "  'annotations': annotations,\n",
        "  'licenses': []\n",
        "  }\n",
        "\n",
        "  with open(os.path.join(ann_dir,f'val.json'),'w') as outfile:\n",
        "    json.dump(coco_dict, outfile, indent=3)\n",
        "\n",
        "\n",
        "# Copy COCO annotations in images folder\n",
        "!cp {os.path.join(ann_dir,'val.json')} {val_img_dir}\n",
        "\n",
        "\n",
        "# Copy COCO annotations in images folder\n",
        "!cp {os.path.join(ann_dir,'val.json')} {val_img_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uara-01_5NIB"
      },
      "source": [
        "### Load Mask dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC4qQcYh5R88"
      },
      "source": [
        "val_ann_file = 'val.json'  # annotations\n",
        "val_ann_path = os.path.join(val_img_dir,val_ann_file)  \n",
        "\n",
        "# Define data transforms\n",
        "transforms = ST.Compose([ST.ToTensor()])\n",
        "\n",
        "# Create dataset\n",
        "val_dataset = CocoDetection(val_img_dir, val_ann_path, transforms = transforms)\n",
        "\n",
        "# Create data loader\n",
        "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=SU.collate_fn)\n",
        "\n",
        "# Get label names\n",
        "if dataset_name == 'MASKD':\n",
        "  label_ids = [1,2]\n",
        "  label_names = ['mask', 'no_mask']\n",
        "elif dataset_name == 'PWMFD':\n",
        "  label_ids = [1,2,3]\n",
        "  label_names = ['with_mask', 'without_mask', 'incorrect_mask']\n",
        "labels = dict(zip(label_ids,label_names))  # Label dictionary with id-name as key-value\n",
        "labels_inv = dict(zip(label_names,label_ids))  # Inverse label dictionary with name-id as key-value\n",
        "label_colors = {1: (0,255,0), 2:(255,0,0), 3: (255,255,0)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKgDZsYwye04"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DibsDgujye05",
        "cellView": "form"
      },
      "source": [
        "!cd {root_dir}\n",
        "\n",
        "# Delete utils package to reload it (if loaded), because YOLOv3 and YOLOv5 have\n",
        "# the same name for it and it causes error\n",
        "try:\n",
        "  sys.modules.pop('utils')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# @markdown Model Selection { display-mode: 'form', run: 'auto' }\n",
        "model_name = 'YOLOv5s' # @param ['SSD300 VGG16', 'SSDlite320 MobileNetV3-Large', 'Faster R-CNN ResNet-50 FPN', 'Faster R-CNN MobileNetV3-Large FPN', 'Mask R-CNN ResNet-50 FPN', 'YOLOv5s', 'YOLOv5m', 'YOLOv5l', 'YOLOv3', 'YOLOv3-tiny', 'YOLOv3-spp']\n",
        "\n",
        "# @markdown *Note: If you get the error \"Cache may be out of date, try 'force_reload=True'\" then restart runtime.*\n",
        "\n",
        "if model_name == 'SSD300 VGG16':\n",
        "  model_id = 'ssd300_vgg16'\n",
        "  model = M.ssd300_vgg16(pretrained=False, progress=True)\n",
        "  model_img_size = (3,300,300)\n",
        "elif model_name == 'SSDlite320 MobileNetV3-Large':\n",
        "  model_id = 'ssdlite320_mobilenet_v3_large'\n",
        "  model = M.ssdlite320_mobilenet_v3_large(pretrained=False, progress=True)\n",
        "  model_img_size = (3,320,320)\n",
        "elif model_name == 'Faster R-CNN ResNet-50 FPN':\n",
        "  model_id = 'fasterrcnn_resnet50_fpn'\n",
        "  model = M.fasterrcnn_resnet50_fpn(pretrained=False, progress=True)\n",
        "  model_img_size = (3,800,800) # COCO's 640x640 in upscaled to the model's minimum 800x800\n",
        "elif model_name == 'Faster R-CNN MobileNetV3-Large FPN':\n",
        "  model_id = 'fasterrcnn_mobilenet_v3_large_fpn'\n",
        "  model = M.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, progress=True)\n",
        "  model_img_size = (3,800,800) \n",
        "elif model_name == 'Mask R-CNN ResNet-50 FPN':\n",
        "  model_id = 'maskrcnn_resnet50_fpn'\n",
        "  model = M.maskrcnn_resnet50_fpn(pretrained=False, progress=True)\n",
        "  model_img_size = (3,800,800)\n",
        "elif 'YOLOv5' in model_name:\n",
        "  model_id = model_name.lower().replace('-','_')\n",
        "  model = torch.hub.load(os.path.join(root_dir,'yolov5'), 'custom', path=load_ckpt_path, source='local', force_reload=True)\n",
        "  model_img_size = (3,640,640)\n",
        "elif 'YOLOv3' in model_name:\n",
        "  model_id = model_name.lower().replace('-','_')\n",
        "  model = torch.hub.load(os.path.join(root_dir,'yolov3'), 'custom', path=load_ckpt_path, source='local', force_reload=True)\n",
        "  model_img_size = (3,640,640)\n",
        "\n",
        "# Prepare model for dataset (for Fast R-CNN or Mask R-CNN)\n",
        "if 'R-CNN' in model_name: \n",
        "  num_classes = len(val_dataset.coco.getCatIds()) + 1\n",
        "  # Get the number of input features for the bbox predictor\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # Replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = M.faster_rcnn.FastRCNNPredictor(in_features, num_classes)  # includes background (0) class\n",
        "  if 'Mask R-CNN' in model_name:\n",
        "    # Get the number of input features for the segmentation max predictor\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # Replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = M.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
        "\n",
        "print('-------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "print(f'Loaded model: {model_name}')\n",
        "model_params = round(sum([param.numel() for param in model.parameters()]) / 1000000, 1)\n",
        "print(f'\\t- Parameters: {model_params}M')\n",
        "model_macs, _ = get_model_complexity_info(model, model_img_size, as_strings=False, \n",
        "                                          print_per_layer_stat=False, verbose=False)\n",
        "model_gflops = round(2 * int(model_macs) / 1000000000, 1)\n",
        "print(f'\\t- GFLOPs: {model_gflops}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Qqfn3WiAqO"
      },
      "source": [
        "### (Optional) Test model with image sample\n",
        "*Note 1: If you get the error \"module 'PIL.TiffTags' has no attribute 'IFD'\" then restart runtime.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCUbgorkiEIU"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Parameters\n",
        "img_from_path = False\n",
        "img_path = '/content/test.jpg'  # Only for image from path\n",
        "img_id = 32  # Only for image not from path (image from val dataset)\n",
        "\n",
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Get image sample\n",
        "if img_from_path:\n",
        "  img = cv2.imread(img_path)\n",
        "else:\n",
        "  img = cv2.imread(os.path.join(val_img_dir,val_dataset.coco.loadImgs([img_id])[0]['file_name']))\n",
        "\n",
        "img = PIL.Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Format image\n",
        "img_tensor = F.convert_image_dtype(F.to_tensor(img),torch.uint8)\n",
        "img_torchvision = torch.div(img_tensor,255).float().to(device)  # Format image for torchvision models\n",
        "img_anns = val_dataset.coco.loadAnns(val_dataset.coco.getAnnIds([img_id]))\n",
        "\n",
        "# Draw ground truth bboxes\n",
        "if not img_from_path:  # If the image is not from path (if it is, ground truth doesn't exist)\n",
        "  true_bboxes = SE.convert_to_xyxy(copy.deepcopy(F.Tensor([obj['bbox'] for obj in img_anns]).to(device)))  # Create deep copy to avoid updating original dataset\n",
        "  true_labels = [labels[obj['category_id']] for obj in img_anns]\n",
        "  true_img = U.draw_bounding_boxes(img_tensor, true_bboxes, true_labels, colors=[label_colors[obj['category_id']] for obj in img_anns])\n",
        "  plt.figure(figsize = (25,7))\n",
        "  plt.title('Ground Truth Detection')\n",
        "  plot = plt.imshow(F.to_pil_image(true_img))\n",
        "\n",
        "# Generate model predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  if 'YOLO' in model_name:\n",
        "    pred = model([img])\n",
        "  else:    \n",
        "    pred = model([img_torchvision])\n",
        "\n",
        "# Get predicted bboxes\n",
        "# For YOLO models\n",
        "if 'YOLO' in model_name:  \n",
        "  pred_bboxes = []\n",
        "  pred_label_ids = []\n",
        "  pred_labels = []\n",
        "  for bbox in pred.xyxy[0]:  # For every bbox\n",
        "    conf = bbox[4]\n",
        "    if conf > th:  # Show only bboxes with high confidence score\n",
        "      pred_bboxes.append(bbox[:4])\n",
        "      label_id = labels_inv[label_names[int(bbox[5])]]  # Convert YOLO label id to COCO label id\n",
        "      pred_label_ids.append(label_id)  \n",
        "      pred_labels.append(labels[label_id] + f'[{int(conf*100)}%]')\n",
        "  if len(pred_bboxes) != 0:\n",
        "    pred_bboxes = torch.stack(pred_bboxes)\n",
        "\n",
        "# For torchvision models\n",
        "else:\n",
        "  for i, bbox in enumerate(pred[0]['boxes']):  # For every bbox\n",
        "    conf = pred[0]['scores'][i]\n",
        "    if conf > th:  # Show only bboxes with high confidence score\n",
        "      pred_bboxes.append(bbox)\n",
        "      label_id = pred[0]['labels'][i]\n",
        "      pred_label_ids.append(label_id)\n",
        "      pred_labels.append(labels[label_id] + f'[{int(conf*100)}%]')\n",
        "  if len(pred_bboxes) != 0:\n",
        "    pred_bboxes = torch.stack(pred_bboxes)\n",
        "\n",
        "# Draw predicted bboxes\n",
        "if len(pred_bboxes) != 0:\n",
        "  pred_img = U.draw_bounding_boxes(img_tensor, pred_bboxes, pred_labels, colors=[label_colors[label_id] for label_id in pred_label_ids])\n",
        "else:  # If no bboxes are found just return the image\n",
        "  pred_img = img_tensor\n",
        "plt.figure(figsize = (25,7))\n",
        "plt.title(f'Predicted Detection (thresh={th})')\n",
        "plot = plt.imshow(F.to_pil_image(pred_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCp8vb53a99T"
      },
      "source": [
        "### Test model on webcam video stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nuiu9DrbAGf"
      },
      "source": [
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# FPS monitor\n",
        "fps = 0\n",
        "\n",
        "# Initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "\n",
        "while True:\n",
        "    \n",
        "    js_reply = video_frame(fps, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convert JS response to PIL Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "    plt.imshow(frame)\n",
        "\n",
        "    # Create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # Format frame (for torchvision models)\n",
        "    frame_tensor = F.convert_image_dtype(F.to_tensor(frame),torch.uint8)\n",
        "    frame_torchvision = torch.div(frame_tensor,255).float().to(device)\n",
        "\n",
        "    # Generate model predictions\n",
        "    model.eval()\n",
        "    model_time = time.time()\n",
        "    with torch.no_grad():\n",
        "      if 'YOLO' in model_name:\n",
        "        pred = model(frame)\n",
        "      else:    \n",
        "        pred = model([frame_torchvision])\n",
        "    model_time = time.time() - model_time\n",
        "    fps = int(1/model_time)\n",
        "\n",
        "    # Get predicted bboxes\n",
        "    # For YOLO models\n",
        "    if 'YOLO' in model_name:  \n",
        "      pred_bboxes = []\n",
        "      pred_label_ids = []\n",
        "      pred_labels = []\n",
        "      for img in pred.xyxy: # For every image\n",
        "        for bbox in img:  # For every bbox of that image\n",
        "          conf = bbox[4]\n",
        "          if conf > th:  # Show only bboxes with high confidence score\n",
        "            pred_bboxes.append(bbox[:4])\n",
        "            label_id = labels_inv[label_names[int(bbox[5])]]  # Convert YOLO label id to COCO label id\n",
        "            pred_label_ids.append(label_id)  \n",
        "            pred_labels.append(labels[label_id] + f'[{int(conf*100)}%]')\n",
        "      if len(pred_bboxes) != 0:\n",
        "        pred_bboxes = torch.stack(pred_bboxes)\n",
        "    # For torchvision models\n",
        "    else:\n",
        "      for i, bbox in enumerate(pred[0]['boxes']):  # For every bbox\n",
        "        conf = pred[0]['scores'][i]\n",
        "        if conf > th:  # Show only bboxes with high confidence score\n",
        "          pred_bboxes.append(bbox)\n",
        "          label_id = pred[0]['labels'][i]\n",
        "          pred_label_ids.append(label_id)\n",
        "          pred_labels.append(labels[label_id] + f'[{int(conf*100)}%]')\n",
        "      if len(pred_bboxes) != 0:\n",
        "        pred_bboxes = torch.stack(pred_bboxes)  \n",
        "\n",
        "    predictions = zip(pred_label_ids, pred_labels, pred_bboxes)\n",
        "\n",
        "    # Loop through detections and draw them on transparent overlay image\n",
        "    for label_id, label, bbox in predictions:\n",
        "      left, top, right, bottom = bbox\n",
        "      left = int(left); top = int(top); right = int(right); bottom = int(bottom)\n",
        "      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), label_colors[label_id], 2)\n",
        "      bbox_array = cv2.putText(bbox_array, \"{}\".format(label),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        label_colors[label_id], 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4E0DKJ4bOg"
      },
      "source": [
        "### Evaluate model\n",
        "*Note 1: If you get the error \"module 'PIL.TiffTags' has no attribute 'IFD'\" then restart runtime.*\n",
        "\n",
        "*Note 2: To get accurate maximum GPU memory usage logging, restart runtime when choosing a different model.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BViZf4Ud4Vrp"
      },
      "source": [
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "print(f'Model: {model_name}')\n",
        "\n",
        "# Evaluate model\n",
        "evaluator, fps, max_mem, outputs = SE.evaluate(model, val_data_loader, device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joangog/object-detection/blob/main/mask_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIjcsMslUx9m"
      },
      "source": [
        "## Model training on mask dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets:\n",
        "*   MASKD (AICrowd)\n",
        "*   Properly Wearing Masked Face Detection Dataset \n",
        "\n",
        "Models\n",
        "*   YOLOv5n\n",
        "*   YOLOv5s\n",
        "*   YOLOv5m"
      ],
      "metadata": {
        "id": "7uSjwcFB2kt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: GPU Runtime needed (hosted or local)**\n",
        "\n",
        "*Example GPU: Tesla K80, 460.32.03, 11441 MiB*"
      ],
      "metadata": {
        "id": "Ix1zP4v82Zb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X0kW-fHdG9R",
        "outputId": "41828a2e-dac0-4f1a-e1fc-f28814812e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, driver_version, memory.total [MiB]\n",
            "Tesla K80, 460.32.03, 11441 MiB\n"
          ]
        }
      ],
      "source": [
        "# Show system specs\n",
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVojvH9i8ua0"
      },
      "source": [
        "### Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lZHe4nxx8raL"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "\n",
        "dataset_name = 'PWMFD'  # 'MASKD' or 'PWMFD'\n",
        "\n",
        "load_ckpt = False  # Flag for whether to continue from existing training checkpoint or not\n",
        "load_cfg = False  # Flag for whether to load custom model configs or not (model config determines model architecture)\n",
        "pretrained = True  # Flag for whether to start from COCO-pretrained model or randomly init weights\n",
        "freeze_layers = True # Flag for freezing layers or not (for YOLO models)\n",
        "\n",
        "img_res = 320  # Image resolution (for YOLO models), try 608, 512, 416, 320 or 224\n",
        "num_workers = 2  #  Data loader workers\n",
        "batch_size = 32  # Data loader batch size\n",
        "\n",
        "mosaic = 1  #  Probability for image mosaic data augmentation (for YOLO models)\n",
        "mixup = 0 #  Probability for image mixup data augmentation, works only when mosaic = 1 (for YOLO models)\n",
        "fl_gamma = 0 # Focal Loss gamma  (for YOLO models)\n",
        " \n",
        "num_epochs = 50  # Epochs\n",
        "\n",
        "frozen_layers = 10  # Layers to freeze (for YOLO models), try 10 (freeze backbone) or 24 (freeze all but last layer)\n",
        "\n",
        "optimizer_type = 'sgd'  # 'sgd' or 'adam'\n",
        "momentum = 0.937  # Optimizer momentum (only for SGD optimizer)\n",
        "weight_decay = 0.0005  # Optimizer weight decay\n",
        "\n",
        "lr = 0.01  # Learning rate (lr_max=lr)\n",
        "lr_f = 0.1  # Learning rate multiplier (lr_min=lr*lr_f)\n",
        "\n",
        "step_size = 3  # (Experimental) Learning rate step size (not used for YOLO, only models that use StepLR)\n",
        "gamma = 0.1  # (Experimental) Learning rate decay (not used for YOLO, only models that use StepLR)\n",
        "\n",
        "\n",
        "# Directories\n",
        "\n",
        "save_ckpt_folder = None  # When None, a name for the model folder is generated automatically\n",
        "load_ckpt_path = ''  # Loaded weights path (used when load_ckpt is True)\n",
        "save_ckpt_dir = '/content/drive/MyDrive/object-detection-checkpoints'  # Model save root directory (used when load_ckpt is False)\n",
        "load_cfg_path  = ''  # For YOLO models (used when load_cfg is True)\n",
        "\n",
        "import os\n",
        "root_dir = os.getcwd()  # Root dir of project\n",
        "dataset_dir = os.path.join(root_dir,f'dataset_{dataset_name}')\n",
        "\n",
        "img_dir = os.path.join(dataset_dir,'images')\n",
        "val_img_dir = os.path.join(img_dir,'val_images')\n",
        "train_img_dir = os.path.join(img_dir,'train_images')\n",
        "\n",
        "label_dir = os.path.join(dataset_dir,'labels')\n",
        "val_label_dir = os.path.join(label_dir,'val_images')\n",
        "train_label_dir = os.path.join(label_dir,'train_images')\n",
        "\n",
        "ann_dir = os.path.join(dataset_dir,'annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9IQxr2jWaui"
      },
      "source": [
        "### Get requirements\n",
        "*Note : Takes about 5 minutes. Restart runtime after installation if a problem arises.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gWOqjlQQdaP"
      },
      "outputs": [],
      "source": [
        "# Clone asset files\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/joangog/object-detection-assets\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','scripts')} ./\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','config')} ./\n",
        "!mv -n {os.path.join(root_dir,'object-detection-assets','requirements.txt')} ./\n",
        "!rm -rf {os.path.join(root_dir,'object-detection-assets')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVa0AxzKlSyv"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!cd {root_dir}\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-XSXNc61vV3"
      },
      "outputs": [],
      "source": [
        "# Install YOLOv5\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/joangog/yolov5.git\n",
        "!pip install -r {os.path.join(root_dir,'yolov5','requirements.txt')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYO5_FOUR1D_"
      },
      "outputs": [],
      "source": [
        "# Install YOLOv3\n",
        "!cd {root_dir}\n",
        "!git clone https://github.com/ultralytics/yolov3\n",
        "!pip install -r {os.path.join(root_dir,'yolov3','requirements.txt')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgn6P8YF3Kv1"
      },
      "outputs": [],
      "source": [
        "# Install unrar command\n",
        "if os.geteuid() != 0:  # If not root, ask for sudo priviledges\n",
        "  from getpass import getpass\n",
        "  password = getpass('Insert sudo password:')\n",
        "  !echo {password} | sudo -S -k apt-get install unrar\n",
        "else:\n",
        "  !apt-get install unrar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp4s5Q5fdG_x"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4eZg3dOcWfsV"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import PIL\n",
        "import IPython\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "import torchvision.models.detection as M\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.utils as U\n",
        "from torchvision.datasets import CocoDetection\n",
        "\n",
        "from pycocotools import coco\n",
        "from pycocotools import mask as cocomask\n",
        "\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "import scripts.utils as SU\n",
        "import scripts.transforms as ST\n",
        "import scripts.engine as SE\n",
        "import scripts.coco_utils as SCU\n",
        "from scripts.coco_eval import CocoEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBNZ1E5oHv2"
      },
      "source": [
        "### Connect to GDrive for storage access\n",
        "*Note: Not possible with local runtime*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gUb3wQZX9R8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFpoV3E0-smW"
      },
      "source": [
        "### Download Mask dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HZmlJ1DpK3ZG"
      },
      "outputs": [],
      "source": [
        "!cd {root_dir}\n",
        "!mkdir -p dataset_{dataset_name}\n",
        "!cd {dataset_dir}\n",
        "!mkdir -p {img_dir} {ann_dir} {label_dir}\n",
        "!cd {img_dir}\n",
        "!mkdir {val_img_dir} {train_img_dir}\n",
        "!cd {label_dir}\n",
        "!mkdir {val_label_dir} {train_label_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLsRgTRRuu3J"
      },
      "outputs": [],
      "source": [
        "if dataset_name == 'MASKD':\n",
        "\n",
        "  !cd {root_dir}\n",
        "\n",
        "  # Download validation images\n",
        "  if not os.path.exists('val_images.zip'):\n",
        "    !gdown --id '101F2k6PJ-tD_uwlsCG7zzGF9ILJW01M1'\n",
        "  !unzip -q -n 'val_images.zip' -d {img_dir}\n",
        "\n",
        "  # Download train images\n",
        "  if not os.path.exists('train_images.zip'):\n",
        "    !gdown --id '1vD_Sxg7dHkB_8OJLsHngBWvp5iGJAETQ'\n",
        "  !unzip -q -n 'train_images.zip' -d {img_dir}\n",
        "\n",
        "  # Download validation annotations\n",
        "  if not os.path.exists('val.json'):\n",
        "    !gdown -O {os.path.join(ann_dir,'val.json')} --id '1YLV7-7vmiNdFI8Xpdx_jbhnxfgQRWrgF'\n",
        "\n",
        "  # Download train annotations\n",
        "  if not os.path.exists('train.json'):\n",
        "    !gdown -O {os.path.join(ann_dir,'train.json')}  --id '1AqeDJps-aZ743vFJ6p2_RjtSFjPtIOtD'\n",
        "\n",
        "elif dataset_name == 'PWMFD':\n",
        "\n",
        "  # Download validation images\n",
        "  if not os.path.exists('val_images.rar'):\n",
        "    !gdown -O 'val_images.rar' --id  1ZXuSwoRvTnnca81RUj3kMoLFZJ6auAwT\n",
        "  !unrar e -idq -o- 'val_images.rar' -d {val_img_dir}\n",
        "\n",
        "  # Download train images\n",
        "  if not os.path.exists('train_images.rar'):\n",
        "    !gdown -O 'train_images.rar' --id  16uI5ZEiq2JEYBH4_DhmxVAHSqzY5UQ1b\n",
        "  !unrar e -idq -o- 'train_images.rar' -d {train_img_dir} \n",
        "\n",
        "  # Convert annotation files from PASCAL VOC .xml to COCO .json (only for PWMFD dataset)\n",
        "  \n",
        "  label_ids = {'with_mask': 1, 'without_mask': 2, 'incorrect_mask': 3}  # BG class is 0\n",
        "    \n",
        "  for type_img in ['val', 'train']:\n",
        "\n",
        "    ann_count = 0  # Annotation counter\n",
        "\n",
        "    images = []\n",
        "    categories = []\n",
        "    annotations = []\n",
        "\n",
        "    xml_files = os.listdir(os.path.join(os.path.join(img_dir,f'{type_img}_images')))\n",
        "    xml_files = [file for file in xml_files if '.xml' in file]\n",
        "\n",
        "    # Categories\n",
        "    for label in label_ids:\n",
        "      categories.append(\n",
        "          {\n",
        "            'supercategory': 'none',\n",
        "            'id': label_ids[label],\n",
        "            'name': label\n",
        "          }\n",
        "      )\n",
        "\n",
        "    for xml_file in xml_files:  # For each annotation file\n",
        "\n",
        "      tree = ET.parse(os.path.join(os.path.join(img_dir,f'{type_img}_images'),xml_file))\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # Image\n",
        "      file_name = root[0].text\n",
        "      height = int(root[1][1].text)\n",
        "      width = int(root[1][0].text)\n",
        "      id = int(re.sub(r'^\\D*0*', '', file_name).replace('.jpg',''))\n",
        "      images.append(\n",
        "          {\n",
        "            'file_name': file_name,\n",
        "            'height': height,\n",
        "            'width': width,\n",
        "            'id': id\n",
        "          }\n",
        "      )\n",
        "      \n",
        "      # Annotations\n",
        "      if len(root) > 2:  # If annotations (object attribute) exist, they will be after the 1-index attribute in the XML\n",
        "        for i in range(2,len(root)):\n",
        "          category_id = label_ids[root[i][0].text]\n",
        "          xmin = int(root[i][1][0].text)\n",
        "          ymin = int(root[i][1][1].text)\n",
        "          xmax = int(root[i][1][2].text)\n",
        "          ymax = int(root[i][1][3].text)\n",
        "          annotations.append(\n",
        "              {\n",
        "                'iscrowd': 0,\n",
        "                'image_id': id,\n",
        "                'bbox': [xmin, ymin, xmax-xmin, ymax-ymin],\n",
        "                'area': (xmax-xmin) * (ymax-ymin),\n",
        "                'category_id': category_id,\n",
        "                'ignore': 0,\n",
        "                'id': ann_count,\n",
        "                'segmentation': []\n",
        "              }\n",
        "          )\n",
        "          ann_count += 1\n",
        "\n",
        "    coco_dict = {\n",
        "    'info': {},\n",
        "    'images': images,\n",
        "    'categories': categories,\n",
        "    'annotations': annotations,\n",
        "    'licenses': []\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(ann_dir,f'{type_img}.json'),'w') as outfile:\n",
        "      json.dump(coco_dict, outfile, indent=3)\n",
        "\n",
        "\n",
        "# Copy COCO annotations in images folder\n",
        "!cp {os.path.join(ann_dir,'val.json')} {val_img_dir}\n",
        "!cp {os.path.join(ann_dir,'train.json')} {train_img_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqD0UEvTBQsD"
      },
      "source": [
        "### (Only for YOLO models) Convert format of annotations from COCO to YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPgZDxhEBPyZ"
      },
      "outputs": [],
      "source": [
        "for type_images in ['val', 'train']:\n",
        "\n",
        "    ann_path = os.path.join(ann_dir, type_images + \".json\")\n",
        "    dataset = coco.COCO(ann_path)\n",
        "    img_ids = dataset.getImgIds()\n",
        "\n",
        "    for img_id in img_ids:\n",
        "\n",
        "      img_anns = dataset.loadAnns(dataset.getAnnIds([img_id]))\n",
        "      img_data = dataset.loadImgs([img_id])[0]\n",
        "      img_file = img_data['file_name']\n",
        "      img_width = img_data['width']\n",
        "      img_height = img_data['height']\n",
        "\n",
        "      label_file = img_file.replace('.jpg','.txt')\n",
        "      \n",
        "      with open(os.path.join(label_dir, f'{type_images}_images', label_file), 'w') as outfile:\n",
        "        for ann in img_anns:\n",
        "          x_center = (ann['bbox'][0] + ann['bbox'][2]/2) / img_width  # convert x_min to x_center and normalize to [0,1]\n",
        "          y_center = (ann['bbox'][1] + ann['bbox'][3]/2) / img_height  # convert y_min to y_center and normalize to [0,1]\n",
        "          width = ann['bbox'][2] / img_width\n",
        "          height = ann['bbox'][3] / img_height \n",
        "          outfile.write(\"{} {} {} {} {}\\n\".format(int(ann['category_id'])-1,x_center,y_center,width,height))   # Category ids must be 0-indexed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uara-01_5NIB"
      },
      "source": [
        "### Load Mask dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC4qQcYh5R88"
      },
      "outputs": [],
      "source": [
        "val_ann_file = 'val.json'  # annotations\n",
        "val_ann_path = os.path.join(val_img_dir,val_ann_file)  \n",
        "\n",
        "train_ann_file = 'train.json'  # annotations\n",
        "train_ann_path = os.path.join(train_img_dir,train_ann_file)  \n",
        "\n",
        "# Define data transforms\n",
        "transforms = ST.Compose([ST.ToTensor()])\n",
        "\n",
        "# Create datasets\n",
        "val_dataset = CocoDetection(val_img_dir, val_ann_path, transforms = transforms)\n",
        "train_dataset = CocoDetection(train_img_dir, train_ann_path, transforms = transforms)\n",
        "\n",
        "# Create data loaders\n",
        "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=SU.collate_fn)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=SU.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwGEoyOZfKq"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865,
          "referenced_widgets": [
            "f537e631d344401eb78c0703538b4b20",
            "a942b561d976476fbf2eb55f3afd1b38",
            "e1035a8ca98945708a7cc57dbe526a55",
            "5d1dd9c83014436695c9a658faca2fb3",
            "a41547c49f824da9b38bdb32464043cc",
            "a10481b971ea47d7a3bced78b69b0b18",
            "0c82a1745c75487d93b6af4686391723",
            "4d6c52da08954bd7a9a88942e0bbd185",
            "03d752b14dfe435cac076b5876a4cd79",
            "d0d7ad7e23234268835e740b7e9f6e43",
            "d6c7832614384369b47cefa0187c4c71"
          ]
        },
        "id": "ZVJqPlW8dTvc",
        "outputId": "5f3bc4c3-a4d0-4cd3-beac-439c40d8b688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m matplotlib>=3.2.2 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2) (1.15.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m torch>=1.7.0 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0) (3.7.4.3)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m torchvision>=0.8.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.1) (7.1.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.1) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision>=0.8.1) (3.7.4.3)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m pandas>=1.1.4 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.4) (1.15.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 5 packages updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 2022-3-13 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f537e631d344401eb78c0703538b4b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loaded model: YOLOv5s\n",
            "\t- Parameters: 7.2M\n",
            "\t- GFLOPs: 4.1\n"
          ]
        }
      ],
      "source": [
        "# Delete utils package to reload it (if loaded), because YOLOv3 and YOLOv5 have\n",
        "# the same name for it and it causes error\n",
        "try:\n",
        "  sys.modules.pop('utils')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# @markdown Model Selection { display-mode: 'form', run: 'auto' }\n",
        "model_name = 'YOLOv5s' # @param ['YOLOv5m', 'YOLOv5s', 'YOLOv5n']\n",
        "\n",
        "# @markdown *Note: If you get the error \"Cache may be out of date, try 'force_reload=True'\" then restart runtime.*\n",
        "\n",
        "if model_name == 'SSD300 VGG16':\n",
        "  model_id = 'ssd300_vgg16'\n",
        "  model = M.ssd300_vgg16(pretrained=True, progress=True)\n",
        "  model_img_size = (3,300,300)\n",
        "elif model_name == 'SSDlite320 MobileNetV3-Large':\n",
        "  model_id = 'ssdlite320_mobilenet_v3_large'\n",
        "  model = M.ssdlite320_mobilenet_v3_large(pretrained=True, progress=True)\n",
        "  model_img_size = (3,320,320)\n",
        "elif model_name == 'Faster R-CNN ResNet-50 FPN':\n",
        "  model_id = 'fasterrcnn_resnet50_fpn'\n",
        "  model = M.fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,800,800) # COCO's 640x640 in upscaled to the model's minimum 800x800\n",
        "elif model_name == 'Faster R-CNN MobileNetV3-Large FPN':\n",
        "  model_id = 'fasterrcnn_mobilenet_v3_large_fpn'\n",
        "  model = M.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,800,800) \n",
        "elif model_name == 'Mask R-CNN ResNet-50 FPN':\n",
        "  model_id = 'maskrcnn_resnet50_fpn'\n",
        "  model = M.maskrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "  model_img_size = (3,800,800)\n",
        "elif model_name == 'YOLOv5n':\n",
        "  model_id = 'yolov5n'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5n', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv5s':\n",
        "  model_id = 'yolov5s'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv5m':\n",
        "  model_id = 'yolov5m'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5m', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv5l':\n",
        "  model_id = 'yolov5l'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5l', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv3':\n",
        "  model_id = 'yolov3'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv3-tiny':\n",
        "  model_id = 'yolov3_tiny'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3_tiny', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "elif model_name == 'YOLOv3-spp':\n",
        "  model_id = 'yolov3_spp'\n",
        "  model = torch.hub.load('ultralytics/yolov3', 'yolov3_spp', force_reload=True)\n",
        "  model_img_size = (3,img_res,img_res)\n",
        "\n",
        "print('-------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "print(f'Loaded model: {model_name}')\n",
        "model_params = round(sum([param.numel() for param in model.parameters()]) / 1000000, 1)\n",
        "print(f'\\t- Parameters: {model_params}M')\n",
        "model_macs, _ = get_model_complexity_info(model, model_img_size, as_strings=False, \n",
        "                                          print_per_layer_stat=False, verbose=False)\n",
        "model_gflops = round(2 * int(model_macs) / 1000000000, 1)\n",
        "print(f'\\t- GFLOPs: {model_gflops}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0hskTBqjSOj"
      },
      "source": [
        "### (Optional) Open Tensorboard Monitor\n",
        "*Note: Tensorboard may not show properly in some browsers and operating systems.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qyI5lrXoMw9K",
        "outputId": "aaf6d812-a9a9-4ebb-c886-ef1a5750159c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load Tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --bind_all --logdir {save_ckpt_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvGK5hUw-q_k"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94J_kCy--zPp"
      },
      "outputs": [],
      "source": [
        "# Set model checkpoint folder\n",
        "start_time = int(time.time())\n",
        "if not save_ckpt_folder:\n",
        "  save_ckpt_folder = f'{dataset_name}_{model_id}_{start_time}_run'\n",
        "\n",
        "# Prepare model for new dataset (for Fast R-CNN or Mask R-CNN)\n",
        "if 'R-CNN' in model_name: \n",
        "  num_classes = len(val_dataset.coco.getCatIds())+1  # includes background (0) class\n",
        "  # Get the number of input features for the bbox predictor\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # Replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = M.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "  if 'Mask R-CNN' in model_name:\n",
        "    # Get the number of input features for the segmentation max predictor\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # Replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = M.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
        "\n",
        "# Model Training \n",
        "\n",
        "if 'YOLO' in model_name: # For YOLO models\n",
        "\n",
        "  yolo_version = int(model_id[5])\n",
        "\n",
        "  # Generate hyperparameter file\n",
        "  hyp_path = os.path.join(save_ckpt_dir,save_ckpt_folder,'hyp.yaml')\n",
        "  !cp {root_dir}/yolov{yolo_version}/data/hyps/hyp.scratch.yaml 'hyp.yaml'\n",
        "  with open('hyp.yaml','a') as outfile:\n",
        "    outfile.write(\n",
        "\n",
        "f\"\"\"lr0: {lr:.9f}\n",
        "lrf: {lr_f:.9f}\n",
        "momentum: {momentum}\n",
        "weight_decay: {weight_decay}\n",
        "mosaic: {mosaic}\n",
        "mixup: {mixup}\n",
        "fl_gamma: {fl_gamma}\n",
        "\"\"\"\n",
        "# .9f is for suppressing scientific notation (e.g. 2e10) for float numbers\n",
        "\n",
        ")\n",
        "\n",
        "  train_script_path = os.path.join(root_dir,f'yolov{yolo_version}','train.py')\n",
        "  data_path = os.path.join(root_dir,'config',f'{dataset_name}_yolov{yolo_version}.yaml')\n",
        "\n",
        "  if load_ckpt:  # If load a pre-existing checkpoint, resume from it\n",
        "    temp_save_ckpt_folder = re.search(r'/' + dataset_name + '[^/]*/', load_ckpt_path).group(0).replace('/','')  # Get the save folder name from the loaded folder name\n",
        "    temp_load_ckpt_path = load_ckpt_path\n",
        "  else:  # If not load a pre-existing checkpoint, load pre-trained COCO weights or randomly initialize weights\n",
        "    if pretrained:\n",
        "      temp_save_ckpt_folder = save_ckpt_folder\n",
        "      temp_load_ckpt_path = f'{model_id.replace(\"_\",\"-\")}.pt'\n",
        "    else:\n",
        "      temp_save_ckpt_folder = save_ckpt_folder\n",
        "      temp_load_ckpt_path = '\"\"'\n",
        "  if load_cfg:  # If load custom model config\n",
        "    temp_cfg_path = load_cfg_path\n",
        "  else:\n",
        "    temp_cfg_path = model_id.replace(\"_\",\"-\") + '.yaml'\n",
        "\n",
        "  # Run train script\n",
        "  !python {train_script_path} --img {img_res} --epochs {num_epochs} --batch {batch_size} --project {save_ckpt_dir} --name {temp_save_ckpt_folder} --hyp 'hyp.yaml' --data {data_path} \\\n",
        "  {'--resume ' + temp_load_ckpt_path if load_ckpt else '--weights ' + temp_load_ckpt_path} \\\n",
        "  {'--adam ' if optimizer_type == 'adam' else ''} \\\n",
        "  {'--cfg '  +  temp_cfg_path} \\\n",
        "  {'--freeze ' + str(frozen_layers) if freeze_layers else ''}\n",
        "\n",
        "  # Copy hyp.yaml in trained model folder\n",
        "  if not load_ckpt:\n",
        "    !cp 'hyp.yaml' {hyp_path}\n",
        "\n",
        "else: # For all the other models\n",
        "\n",
        "  # Make checkpoint folder\n",
        "  !mkdir {os.path.join(ckpt_dir,ckpt_folder)}\n",
        "\n",
        "  # Load pre-existing checkpoint weights if required\n",
        "  last_epoch = 0\n",
        "  if load_ckpt:\n",
        "    model.load_state_dict(torch.load(load_ckpt_path))\n",
        "    last_epoch = int(re.search(r'epoch[0-9]+', load_ckpt_path).group(0)[5:])\n",
        "    print(f'Loaded checkpoint: {load_ckpt_path}')\n",
        "\n",
        "  # Get appropriate device for model\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  model.to(device)\n",
        "  print(f'Pre-trained Model: {model_name}')\n",
        "\n",
        "  # Construct an optimizer\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  if optimizer_type == 'adam':\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "  elif optimizer_type == 'sgd':\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  # Construct learning rate scheduler\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "  # Save training configs to .txt\n",
        "  hyp_path = os.path.join(save_ckpt_dir,save_ckpt_folder,'hyp.txt')\n",
        "  with open(hyp_path,'w') as outfile:\n",
        "    outfile.write( \n",
        "                         \n",
        "f\"\"\"dataset: {dataset_name}\n",
        "model: {model_id}\n",
        "timestamp: {start_time}\n",
        "last_checkpoint: {load_ckpt_path if load_ckpt else None}\n",
        "num_epochs: {num_epochs}\n",
        "batch_size: {batch_size}\n",
        "optimizer: {optimizer_type}\n",
        "learning_rate: {lr}\n",
        "step_size: {step_size}\n",
        "gamma: {gamma}\n",
        "momentum: {momentum}\n",
        "weight_decay: {weight_decay}\"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "  # Train model\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      curr_epoch = last_epoch + epoch + 1\n",
        "\n",
        "      # Train for one epoch, printing every 10 iterations\n",
        "      metric_logger = SE.train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=1)\n",
        "      \n",
        "      # Update the learning rate\n",
        "      lr_scheduler.step()\n",
        "      \n",
        "      # Evaluate on the test dataset\n",
        "      evaluator, _, _, _ = SE.evaluate(model, val_data_loader, device)\n",
        "\n",
        "      # Save model checkpoint\n",
        "      ckpt_file = f'epoch{curr_epoch}'\n",
        "      ckpt_path = os.path.join(ckpt_dir,ckpt_folder,ckpt_file)\n",
        "      torch.save(model.state_dict(), ckpt_path)\n",
        "\n",
        "      # Delete previous model checkpoint\n",
        "      previous_ckpt_file = f'epoch{curr_epoch-1}'\n",
        "      if os.path.exists(previous_ckpt_file):\n",
        "        os.remove(previous_ckpt_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "l9IQxr2jWaui",
        "Lp4s5Q5fdG_x",
        "LBR6LGJI2nbe",
        "kFpoV3E0-smW",
        "TqD0UEvTBQsD",
        "Uara-01_5NIB"
      ],
      "name": "mask_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f537e631d344401eb78c0703538b4b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a942b561d976476fbf2eb55f3afd1b38",
              "IPY_MODEL_e1035a8ca98945708a7cc57dbe526a55",
              "IPY_MODEL_5d1dd9c83014436695c9a658faca2fb3"
            ],
            "layout": "IPY_MODEL_a41547c49f824da9b38bdb32464043cc"
          }
        },
        "a942b561d976476fbf2eb55f3afd1b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a10481b971ea47d7a3bced78b69b0b18",
            "placeholder": "​",
            "style": "IPY_MODEL_0c82a1745c75487d93b6af4686391723",
            "value": "100%"
          }
        },
        "e1035a8ca98945708a7cc57dbe526a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d6c52da08954bd7a9a88942e0bbd185",
            "max": 14808437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03d752b14dfe435cac076b5876a4cd79",
            "value": 14808437
          }
        },
        "5d1dd9c83014436695c9a658faca2fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0d7ad7e23234268835e740b7e9f6e43",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c7832614384369b47cefa0187c4c71",
            "value": " 14.1M/14.1M [00:00&lt;00:00, 74.4MB/s]"
          }
        },
        "a41547c49f824da9b38bdb32464043cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10481b971ea47d7a3bced78b69b0b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c82a1745c75487d93b6af4686391723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d6c52da08954bd7a9a88942e0bbd185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03d752b14dfe435cac076b5876a4cd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0d7ad7e23234268835e740b7e9f6e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c7832614384369b47cefa0187c4c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
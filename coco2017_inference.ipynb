{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "coco2017_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AJ-lfNPQlcuO",
        "kFpoV3E0-smW"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28993902d4e147e2b03dad2eaef48f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b3da927bab5f49d7b1d439965eb63ba1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d8d0168f1280406199da988c968eddd0",
              "IPY_MODEL_113cbdfa54a54f88ab877bbc12836256"
            ]
          }
        },
        "b3da927bab5f49d7b1d439965eb63ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8d0168f1280406199da988c968eddd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b466c6b0d49c4d2fbd06c7921c72a355",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a994a35d9b74cef803680c756671da0"
          }
        },
        "113cbdfa54a54f88ab877bbc12836256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ce6da6d4b0764da48400c4d951865b06",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:02&lt;00:00, 64.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c69418d9b154f5789792fabee1914a1"
          }
        },
        "b466c6b0d49c4d2fbd06c7921c72a355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a994a35d9b74cef803680c756671da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce6da6d4b0764da48400c4d951865b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c69418d9b154f5789792fabee1914a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joangog/object-detection/blob/main/coco2017_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIjcsMslUx9m"
      },
      "source": [
        "## Model comparison on COCO 2017 dataset\n",
        "*   SSD300 VGG16\n",
        "*   Faster R-CNN ResNet-50 FPN\n",
        "*   Mask R-CNN ResNet-50 FPN\n",
        "*   YOLOv5s\n",
        "*   YOLOv5m\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9IQxr2jWaui"
      },
      "source": [
        "### Get requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-XSXNc61vV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762b79f5-9cb3-4cad-fc13-597e7d753580"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install Yolov5\n",
        "cd /content\n",
        "git clone https://github.com/ultralytics/yolov5\n",
        "cd yolov5\n",
        "pip install --quiet -r requirements.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gWOqjlQQdaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2bd96e-eaa4-42e6-f29a-cd7cd74ebbc5"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Clone asset files\n",
        "cd /content\n",
        "git clone https://github.com/joangog/object-detection-assets\n",
        "cd object-detection-assets\n",
        "mv assets ../\n",
        "rm -rf /content/object-detection-assets/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'object-detection-assets'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 11 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGQL4FnNa94s"
      },
      "source": [
        "cp /content/vision/references/detection/utils.py ../"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp4s5Q5fdG_x"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZg3dOcWfsV"
      },
      "source": [
        "# Import packages\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import ssd300_vgg16, fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "import assets.utils\n",
        "from assets.utils import collate_fn\n",
        "from assets.transforms import Compose, ToTensor\n",
        "from assets.coco_utils import get_coco_api_from_dataset\n",
        "from assets.coco_eval import CocoEvaluator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ-lfNPQlcuO"
      },
      "source": [
        "### Define aux functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31FW_LJclb-P"
      },
      "source": [
        "# Copied and adjusted from repo pytorch/vision/references/detection/engine.py\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = assets.utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [sample for target in targets for sample in target]  # Added this line\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator, outputs\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFpoV3E0-smW"
      },
      "source": [
        "### Download COCO 2017 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6rHTrcZ1Wle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1278af06-74e7-4f4f-f5fe-f20da7e90ecb"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download images\n",
        "cd /content\n",
        "wget 'http://images.cocodataset.org/zips/val2017.zip'\n",
        "unzip -q 'val2017.zip'\n",
        "rm 'val2017.zip'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-09 08:01:06--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.14.188\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.14.188|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  15.9MB/s    in 50s     \n",
            "\n",
            "2021-08-09 08:01:57 (15.4 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIvrZuC_1YK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d268f048-3734-4190-865d-2d3fe08733bc"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download annotations\n",
        "cd /content\n",
        "wget 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "unzip -q 'annotations_trainval2017.zip'\n",
        "rm 'annotations_trainval2017.zip'\n",
        "cp '/content/annotations/instances_val2017.json' '/content/val2017'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-09 08:02:03--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.110.252\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.110.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  16.3MB/s    in 16s     \n",
            "\n",
            "2021-08-09 08:02:20 (14.8 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uara-01_5NIB"
      },
      "source": [
        "### Load COCO 2017 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC4qQcYh5R88",
        "outputId": "657ada73-1233-4432-842a-99a0013c5944"
      },
      "source": [
        "img_dir = '/content/val2017'\n",
        "ann_file = os.path.join(img_dir,'instances_val2017.json')  # annotations\n",
        "\n",
        "# Define data transforms\n",
        "transforms = Compose([ToTensor()]) # Compose([Resize((640,640)), ToTensor()])\n",
        "\n",
        "# Create dataset\n",
        "dataset = CocoDetection(img_dir, ann_file, transforms = transforms)\n",
        "\n",
        "# Create data loader\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.71s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwGEoyOZfKq"
      },
      "source": [
        "### Load pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "28993902d4e147e2b03dad2eaef48f4d",
            "b3da927bab5f49d7b1d439965eb63ba1",
            "d8d0168f1280406199da988c968eddd0",
            "113cbdfa54a54f88ab877bbc12836256",
            "b466c6b0d49c4d2fbd06c7921c72a355",
            "3a994a35d9b74cef803680c756671da0",
            "ce6da6d4b0764da48400c4d951865b06",
            "7c69418d9b154f5789792fabee1914a1"
          ]
        },
        "id": "ZVJqPlW8dTvc",
        "cellView": "form",
        "outputId": "3be9f018-b34b-4330-ad30-8f94fa6759a5"
      },
      "source": [
        "%cd /content\n",
        "\n",
        "# @markdown Model Selection { display-mode: 'form', run: 'auto' }\n",
        "model_name = 'Mask R-CNN ResNet-50 FPN' # @param ['SSD300 VGG16', 'Faster R-CNN ResNet-50 FPN', 'Mask R-CNN ResNet-50 FPN', 'YOLOv5s', 'YOLOv5m']\n",
        "\n",
        "if model_name == 'SSD300 VGG16':\n",
        "  model_id = 'ssd'\n",
        "  model = ssd300_vgg16(pretrained=True, progress=True)\n",
        "elif model_name == 'Faster R-CNN ResNet-50 FPN':\n",
        "  model_id = 'fasterrcnn'\n",
        "  model = fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "elif model_name == 'Mask R-CNN ResNet-50 FPN':\n",
        "  model_id = 'maskrcnn'\n",
        "  model = maskrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "elif model_name == 'YOLOv5s':\n",
        "  model_id = 'yolov5s'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "elif model_name == 'YOLOv5m':\n",
        "  model_id = 'yolov5m'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'yolov5m')\n",
        "\n",
        "print('Loaded model: '+ model_name + '\\n')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28993902d4e147e2b03dad2eaef48f4d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded model: Mask R-CNN ResNet-50 FPN\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4E0DKJ4bOg"
      },
      "source": [
        "### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BViZf4Ud4Vrp",
        "outputId": "83d32ceb-e093-4856-c7a0-526ab36488f1"
      },
      "source": [
        "# Get appropriate device for model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator, outputs = evaluate(model, data_loader, device)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test:  [   0/2500]  eta: 0:54:30  model_time: 1.0786 (1.0786)  evaluator_time: 0.0193 (0.0193)  time: 1.3082  data: 0.2074  max mem: 1075\n",
            "Test:  [ 100/2500]  eta: 0:36:14  model_time: 0.7791 (0.8475)  evaluator_time: 0.0300 (0.0500)  time: 0.8627  data: 0.0042  max mem: 1494\n",
            "Test:  [ 200/2500]  eta: 0:34:33  model_time: 0.8005 (0.8447)  evaluator_time: 0.0432 (0.0495)  time: 0.8966  data: 0.0041  max mem: 1494\n",
            "Test:  [ 300/2500]  eta: 0:33:04  model_time: 0.8300 (0.8444)  evaluator_time: 0.0328 (0.0503)  time: 0.9361  data: 0.0036  max mem: 1494\n",
            "Test:  [ 400/2500]  eta: 0:31:41  model_time: 0.7782 (0.8481)  evaluator_time: 0.0450 (0.0503)  time: 0.8937  data: 0.0037  max mem: 1495\n",
            "Test:  [ 500/2500]  eta: 0:30:07  model_time: 0.8243 (0.8466)  evaluator_time: 0.0311 (0.0503)  time: 0.9139  data: 0.0040  max mem: 1558\n",
            "Test:  [ 600/2500]  eta: 0:28:41  model_time: 0.8214 (0.8489)  evaluator_time: 0.0539 (0.0506)  time: 0.9323  data: 0.0039  max mem: 1558\n",
            "Test:  [ 700/2500]  eta: 0:27:12  model_time: 0.8478 (0.8492)  evaluator_time: 0.0373 (0.0508)  time: 0.9326  data: 0.0045  max mem: 1558\n",
            "Test:  [ 800/2500]  eta: 0:25:46  model_time: 0.8650 (0.8509)  evaluator_time: 0.0560 (0.0511)  time: 0.9658  data: 0.0040  max mem: 1558\n",
            "Test:  [ 900/2500]  eta: 0:24:14  model_time: 0.7599 (0.8506)  evaluator_time: 0.0347 (0.0507)  time: 0.8720  data: 0.0040  max mem: 1558\n",
            "Test:  [1000/2500]  eta: 0:22:44  model_time: 0.8111 (0.8513)  evaluator_time: 0.0511 (0.0508)  time: 0.9227  data: 0.0040  max mem: 1558\n",
            "Test:  [1100/2500]  eta: 0:21:14  model_time: 0.8539 (0.8523)  evaluator_time: 0.0324 (0.0506)  time: 0.9198  data: 0.0037  max mem: 1558\n",
            "Test:  [1200/2500]  eta: 0:19:42  model_time: 0.8289 (0.8525)  evaluator_time: 0.0372 (0.0500)  time: 0.9249  data: 0.0044  max mem: 1558\n",
            "Test:  [1300/2500]  eta: 0:18:12  model_time: 0.8123 (0.8535)  evaluator_time: 0.0343 (0.0498)  time: 0.8697  data: 0.0042  max mem: 1558\n",
            "Test:  [1400/2500]  eta: 0:16:41  model_time: 0.8395 (0.8536)  evaluator_time: 0.0403 (0.0498)  time: 0.9149  data: 0.0043  max mem: 1558\n",
            "Test:  [1500/2500]  eta: 0:15:09  model_time: 0.7741 (0.8529)  evaluator_time: 0.0366 (0.0498)  time: 0.8857  data: 0.0038  max mem: 1558\n",
            "Test:  [1600/2500]  eta: 0:13:38  model_time: 0.8779 (0.8527)  evaluator_time: 0.0359 (0.0496)  time: 0.9400  data: 0.0045  max mem: 1558\n",
            "Test:  [1700/2500]  eta: 0:12:08  model_time: 0.7862 (0.8532)  evaluator_time: 0.0253 (0.0499)  time: 0.8645  data: 0.0045  max mem: 1558\n",
            "Test:  [1800/2500]  eta: 0:10:37  model_time: 0.8360 (0.8532)  evaluator_time: 0.0386 (0.0501)  time: 0.9321  data: 0.0040  max mem: 1558\n",
            "Test:  [1900/2500]  eta: 0:09:06  model_time: 0.8826 (0.8533)  evaluator_time: 0.0307 (0.0501)  time: 0.9728  data: 0.0035  max mem: 1558\n",
            "Test:  [2000/2500]  eta: 0:07:34  model_time: 0.8070 (0.8530)  evaluator_time: 0.0362 (0.0499)  time: 0.9073  data: 0.0042  max mem: 1558\n",
            "Test:  [2100/2500]  eta: 0:06:04  model_time: 0.8002 (0.8528)  evaluator_time: 0.0428 (0.0502)  time: 0.9170  data: 0.0044  max mem: 1558\n",
            "Test:  [2200/2500]  eta: 0:04:32  model_time: 0.8153 (0.8527)  evaluator_time: 0.0347 (0.0501)  time: 0.8986  data: 0.0038  max mem: 1558\n",
            "Test:  [2300/2500]  eta: 0:03:01  model_time: 0.8203 (0.8524)  evaluator_time: 0.0483 (0.0502)  time: 0.9037  data: 0.0045  max mem: 1558\n",
            "Test:  [2400/2500]  eta: 0:01:30  model_time: 0.8506 (0.8519)  evaluator_time: 0.0368 (0.0502)  time: 0.9454  data: 0.0038  max mem: 1627\n",
            "Test:  [2499/2500]  eta: 0:00:00  model_time: 0.8032 (0.8511)  evaluator_time: 0.0326 (0.0501)  time: 0.9074  data: 0.0043  max mem: 1627\n",
            "Test: Total time: 0:37:50 (0.9082 s / it)\n",
            "Averaged stats: model_time: 0.8032 (0.8511)  evaluator_time: 0.0326 (0.0501)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=4.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=4.19s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.030\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.085\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.088\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.083\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.140\n",
            "IoU metric: segm\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.035\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.027\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.059\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.079\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.082\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.077\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PhLjHCmTpDS"
      },
      "source": [
        "## Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY3apojFTt2f"
      },
      "source": [
        "results = []\n",
        "for i, img_id in enumerate(img_ids):  # For every image\n",
        "  img_predictions = predictions[i]\n",
        "  pred_boxes = img_predictions['boxes'].tolist()\n",
        "  pred_labels = img_predictions['labels'].tolist()\n",
        "  pred_scores = img_predictions['scores'].tolist()\n",
        "  for j in range(0,len(pred_box)):  # For every predicted object\n",
        "    pred_box = pred_boxes[j]\n",
        "    results.append({\n",
        "        'image_id': img_id,\n",
        "        'category_id': pred_labels[j],\n",
        "        'bbox': [round(val,1) for val in pred_box],  # Round for lower file size\n",
        "        'score': pred_scores[j]\n",
        "        })\n",
        "\n",
        "results_file = os.path.join(img_dir,f'{model_id}_coco17_results.json')\n",
        "with open(results_file, 'w') as outfile: \n",
        "    json.dump(results, outfile,indent = 2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "y7eEGk6qj2D4",
        "outputId": "4055676b-f257-46e0-aa26-2bb37baaef2f"
      },
      "source": [
        "files.download(results_file) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_036e8f50-f626-49a5-ae78-0da65a1dd820\", \"fasterrcnn_coco17_results.json\", 820)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsk8d7xeS-MR"
      },
      "source": [
        "# results_dataset = dataset.loadRes(results_file)\n",
        "# eval = cocoeval.COCOeval(dataset, results_dataset, iouType='bbox')\n",
        "# eval.evaluate()\n",
        "# eval.accumulate()\n",
        "# eval.summarize()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}